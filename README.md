# Projet-MLA

# Reproduction du mod√®le CamemBERT

Ce projet vise √† reproduire les r√©sultats exp√©rimentaux de l'article *CamemBERT: a Tasty French Language Model*. Il s'agit d'impl√©menter en TensorFlow ou PyTorch l'architecture propos√©e et de valider ses performances sur plusieurs t√¢ches NLP en utilisant un GPU.

---

## üìã **Objectifs du projet :**
- Impl√©menter l'architecture CamemBERT (bas√©e sur RoBERTa) pour le fran√ßais.
- Entra√Æner le mod√®le sur des corpus diversifi√©s (OSCAR).
- √âvaluer les performances sur des t√¢ches de NLP : POS tagging, parsing, NER, et NLI.
- Comparer les r√©sultats avec ceux de l'article original.

---

## üõ†Ô∏è **Pr√©requis :**
### **Environnement de d√©veloppement :**
- **Langages** : Python 3.8+
- **Frameworks** : TensorFlow / PyTorch
- **GPU** : CUDA / cuDNN install√©s pour l'acc√©l√©ration mat√©rielle

### **Biblioth√®ques n√©cessaires :**
```bash
pip install torch torchvision transformers
pip install tensorflow
pip install sentencepiece
```

---

## üìÇ **Structure du projet :**
```plaintext
CamemBERT-Reproduction/
‚îÇ
‚îú‚îÄ‚îÄ data/                  # Donn√©es pr√©trait√©es (OSCAR, CCNet, etc.)
‚îú‚îÄ‚îÄ models/                # Enregistrements des mod√®les entra√Æn√©s
‚îú‚îÄ‚îÄ scripts/               # Scripts d'entra√Ænement et d'√©valuation
‚îú‚îÄ‚îÄ notebooks/             # Analyse des r√©sultats et visualisations
‚îú‚îÄ‚îÄ report/                # Rapport LaTeX (6-8 pages)
‚îî‚îÄ‚îÄ README.md              # Documentation du projet
```

---

## üöÄ **√âtapes de reproduction :**

### 1. **Pr√©traitement des donn√©es :**
- T√©l√©charger le corpus OSCAR fran√ßais.
- Nettoyer et tokenizer avec **SentencePiece**.
  
### 2. **Impl√©mentation du mod√®le :**
- Reproduire l'architecture RoBERTa en utilisant TensorFlow ou PyTorch.
- Impl√©menter le masquage dynamique des mots entiers (Whole-Word Masking).

### 3. **Entra√Ænement :**
- Configurer l'objectif de **Masked Language Modeling (MLM)**.
- Lancer l'entra√Ænement sur un GPU avec des donn√©es de 4 Go et 138 Go :
  ```bash
  python run_pretraining.py --data_path data/oscar_4gb.txt --epochs 10 --batch_size 32
  ```

### 4. **√âvaluation sur des t√¢ches aval :**
- Impl√©menter les t√¢ches POS tagging, NER, parsing, et NLI.
- Utiliser des benchmarks comme **Universal Dependencies (UD)** et **XNLI** :
  ```bash
  python evaluate.py --task pos_tagging --model_path models/camembert_base.pt
  ```

### 5. **Comparaison des r√©sultats :**
- Comparer les performances avec les mod√®les existants (**mBERT**, **XLM-R**).
- Analyser l'impact de la taille et de l'origine du corpus sur les r√©sultats.

### 6. **R√©daction du rapport LaTeX :**
- Pr√©senter l'architecture, les diff√©rences avec l'article, et les r√©sultats obtenus.
- Justifier les choix de conception et discuter les r√©sultats exp√©rimentaux.

### 7. **Cr√©ation de la vid√©o de pr√©sentation :**
- Enregistrer une vid√©o de 5 minutes expliquant le projet et les r√©sultats.
  - **Tous les participants doivent prendre la parole.**

---

## üìà **R√©sultats attendus :**
- Reproduction fid√®le des r√©sultats de l'article.
- Validation des performances sur diff√©rentes t√¢ches NLP.
- Comparaison critique des performances avec les mod√®les de r√©f√©rence.

---

## üí° **Ressources utiles :**
- [Article CamemBERT](https://arxiv.org/abs/1911.03894)
- [Corpus OSCAR](https://oscar-corpus.com/)
- [Documentation Hugging Face](https://huggingface.co/docs)

-------------------------------------------------------------------

# Camembert Implementation

Ce projet impl√©mente une architecture Camembert, un mod√®le de type Transformer bas√© sur RoBERTa, sp√©cialement adapt√© pour le traitement de la langue fran√ßaise. Voici une vue d'ensemble des fichiers et des composants principaux du projet.

## Table des mati√®res
1. [CamembertConfig](#camembertconfig)
2. [Composants du Mod√®le](#composants-du-mod√®le)
   - [CamembertEmbeddings](#camembertembeddings)
   - [CamembertSelfAttention](#camembertselfattention)
   - [CamembertEncoder](#camembertencoder)
   - [CamembertModel](#camembertmodel)
3. [Applications](#applications)
   - [CamembertForPreTraining](#camembertforpretraining)
   - [CamembertForTokenClassification](#camembertfortokenclassification)
4. [Initialisation des poids](#initialisation-des-poids)
5. [Notes suppl√©mentaires](#notes-suppl√©mentaires)

---

## CamembertConfig
La classe `CamembertConfig` d√©finit les hyperparam√®tres du mod√®le. Voici les principaux attributs :

- **`vocab_size`** : Taille du vocabulaire (par d√©faut : 32 000).
- **`hidden_size`** : Dimension des repr√©sentations cach√©es.
- **`num_hidden_layers`** : Nombre de couches dans l'encodeur Transformer.
- **`num_attention_heads`** : Nombre de t√™tes d'attention.
- **`intermediate_size`** : Taille du feed-forward interne dans chaque couche.
- **`hidden_dropout_prob` et `attention_probs_dropout_prob`** : Probabilit√©s de dropout pour √©viter le surapprentissage.
- **`max_position_embeddings`** : Longueur maximale des s√©quences prises en charge.
- **`masking_strategy`** : Strat√©gie de masquage (par d√©faut : `whole_word`).

---

## Composants du Mod√®le

### CamembertEmbeddings
Cette classe g√®re l'incorporation des mots et des positions dans des vecteurs d'embedding.

- Combine les embeddings de mots (`word_embedding`) et de position (`position_embedding`).
- Applique une normalisation par couches (`LayerNorm`) et un dropout.
- Conserve les informations de position et de sens des tokens.

### CamembertSelfAttention
Impl√©mente le m√©canisme d'attention multi-t√™tes.

- Calcule les matrices **Query**, **Key**, et **Value**.
- Effectue un produit scalaire pour calculer les scores d'attention.
- Applique un dropout pour stabiliser l'entra√Ænement.

### CamembertEncoder
Construit l'encodeur complet en empilant plusieurs couches `CamembertLayer`.

- Chaque couche inclut un module d'attention et un module feed-forward.
- Les repr√©sentations sont mises √† jour √† chaque couche.

### CamembertModel
Structure globale combinant les embeddings et l'encodeur Transformer complet.

- Pr√©pare les tenseurs d'entr√©e (id des tokens et masque d'attention).
- Passe les donn√©es √† travers les embeddings et l'encodeur.

---

## Applications

### CamembertForPreTraining
Un mod√®le pour la pr√©-formation avec deux composants principaux :

1. **`CamembertModel`** : Base Transformer pour extraire les repr√©sentations des s√©quences.
2. **`lm_head`** : Une t√™te de pr√©diction du langage (linear layer) pour g√©n√©rer des logits correspondant aux tokens dans le vocabulaire.

### CamembertForTokenClassification
Mod√®le pour des t√¢ches comme l'√©tiquetage de s√©quences (NER, POS tagging, etc.).

- Utilise le `CamembertModel` comme base.
- Ajoute une couche de classification au-dessus pour pr√©dire les √©tiquettes des tokens.
- G√®re les pertes avec une fonction `CrossEntropyLoss`.

---

## Initialisation des poids

La fonction `roberta_init_weights` initialise les poids du mod√®le :

- Les poids des couches lin√©aires et des embeddings sont initialis√©s avec une distribution normale.
- Les biais sont initialis√©s √† z√©ro.
- Les poids du vecteur de padding sont r√©initialis√©s √† z√©ro.

---

## Notes suppl√©mentaires

1. **Embeddings** : Les embeddings des mots sont initialis√©s al√©atoirement puis ajust√©s durant l'entra√Ænement pour capturer le sens des mots et leur contexte.
2. **Attention** : Le m√©canisme d'attention aide le mod√®le √† se concentrer sur les parties importantes de la s√©quence.
3. **R√©f√©rences et ressources** :
   - [Vid√©o sur les encodings positionnels](https://www.youtube.com/watch?v=dichIcUZfOw).
   - Article Medium sur l'attention : [Self-Attention Explained](https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-self-attention-f5fb363c736d).

Ce README fournit une vue d'ensemble pour naviguer et comprendre le projet. Pour des exemples d'utilisation ou des tests, veuillez vous r√©f√©rer √† la documentation ou aux fichiers correspondants.
